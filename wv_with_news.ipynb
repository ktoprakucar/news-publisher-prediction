{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/toprak.ucar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gensim\n",
    "import re\n",
    "import os\n",
    "import logging\n",
    "from h2o.estimators.word2vec import H2OWord2vecEstimator\n",
    "from h2o.estimators import H2OGradientBoostingEstimator\n",
    "from h2o.estimators import H2ORandomForestEstimator\n",
    "import h2o\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['articles1.csv', 'articles3.csv', 'articles2.csv']\n"
     ]
    }
   ],
   "source": [
    "csv_files = [pos_csv for pos_csv in os.listdir(\"data/\") if pos_csv.endswith('.csv')]\n",
    "print(csv_files)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in csv_files:\n",
    "    df = df.append(pd.read_csv(\"data/\" + file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'id', 'title', 'publication', 'author', 'date', 'year',\n",
       "       'month', 'url', 'content'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['author', 'date', 'year', 'month', 'url'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['title'] = df['title'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['publication'] = df['publication'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].str.replace('[^a-zA-Z]', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['content'] = df['content'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>publication</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>atlantic</th>\n",
       "      <td>7179</td>\n",
       "      <td>7179</td>\n",
       "      <td>7179</td>\n",
       "      <td>7179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>breitbart</th>\n",
       "      <td>23781</td>\n",
       "      <td>23781</td>\n",
       "      <td>23781</td>\n",
       "      <td>23781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>business insider</th>\n",
       "      <td>6757</td>\n",
       "      <td>6757</td>\n",
       "      <td>6757</td>\n",
       "      <td>6757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buzzfeed news</th>\n",
       "      <td>4854</td>\n",
       "      <td>4854</td>\n",
       "      <td>4854</td>\n",
       "      <td>4854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cnn</th>\n",
       "      <td>11488</td>\n",
       "      <td>11488</td>\n",
       "      <td>11488</td>\n",
       "      <td>11488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fox news</th>\n",
       "      <td>4354</td>\n",
       "      <td>4354</td>\n",
       "      <td>4354</td>\n",
       "      <td>4354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guardian</th>\n",
       "      <td>8681</td>\n",
       "      <td>8681</td>\n",
       "      <td>8681</td>\n",
       "      <td>8681</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>national review</th>\n",
       "      <td>6203</td>\n",
       "      <td>6203</td>\n",
       "      <td>6203</td>\n",
       "      <td>6203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york post</th>\n",
       "      <td>17493</td>\n",
       "      <td>17493</td>\n",
       "      <td>17493</td>\n",
       "      <td>17493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>new york times</th>\n",
       "      <td>7803</td>\n",
       "      <td>7803</td>\n",
       "      <td>7803</td>\n",
       "      <td>7803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>npr</th>\n",
       "      <td>11992</td>\n",
       "      <td>11992</td>\n",
       "      <td>11992</td>\n",
       "      <td>11992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reuters</th>\n",
       "      <td>10710</td>\n",
       "      <td>10710</td>\n",
       "      <td>10709</td>\n",
       "      <td>10710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>talking points memo</th>\n",
       "      <td>5214</td>\n",
       "      <td>5214</td>\n",
       "      <td>5213</td>\n",
       "      <td>5214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vox</th>\n",
       "      <td>4947</td>\n",
       "      <td>4947</td>\n",
       "      <td>4947</td>\n",
       "      <td>4947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>washington post</th>\n",
       "      <td>11114</td>\n",
       "      <td>11114</td>\n",
       "      <td>11114</td>\n",
       "      <td>11114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Unnamed: 0     id  title  content\n",
       "publication                                           \n",
       "atlantic                   7179   7179   7179     7179\n",
       "breitbart                 23781  23781  23781    23781\n",
       "business insider           6757   6757   6757     6757\n",
       "buzzfeed news              4854   4854   4854     4854\n",
       "cnn                       11488  11488  11488    11488\n",
       "fox news                   4354   4354   4354     4354\n",
       "guardian                   8681   8681   8681     8681\n",
       "national review            6203   6203   6203     6203\n",
       "new york post             17493  17493  17493    17493\n",
       "new york times             7803   7803   7803     7803\n",
       "npr                       11992  11992  11992    11992\n",
       "reuters                   10710  10710  10709    10710\n",
       "talking points memo        5214   5214   5213     5214\n",
       "vox                        4947   4947   4947     4947\n",
       "washington post           11114  11114  11114    11114"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['publication']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142570"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"isBreitbart\"] = np.where(df['publication'] == 'breitbart', \"1\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "for count in range (len(df)):\n",
    "    if (type(df.iloc[count]['content']) != float):\n",
    "        words.append(df.iloc[count]['content'].split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.base_any2vec:consider setting layer size to a multiple of 4 for greater performance\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(words, window=15, \n",
    "                                       size= 50, iter=10, \n",
    "                                       min_count=1, workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ataturk', 0.7852783203125),\n",
       " ('bosporus', 0.7758889198303223),\n",
       " ('ankara', 0.7601418495178223),\n",
       " ('ortakoy', 0.7480058073997498),\n",
       " ('kocarslan', 0.7401304841041565),\n",
       " ('hurriyet', 0.7389910221099854),\n",
       " ('gaziantep', 0.7354176044464111),\n",
       " ('izmir', 0.7345712184906006),\n",
       " ('leyman', 0.7324717044830322),\n",
       " ('turkish', 0.7288885712623596)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"istanbul\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ronaldo', 0.9083605408668518),\n",
       " ('neymar', 0.8756331205368042),\n",
       " ('barcelona', 0.84525066614151),\n",
       " ('higuain', 0.8399900197982788),\n",
       " ('striker', 0.839557409286499),\n",
       " ('atletico', 0.836921215057373),\n",
       " ('cristiano', 0.8350076675415039),\n",
       " ('goalkeeper', 0.8214976191520691),\n",
       " ('juve', 0.8200994729995728),\n",
       " ('isco', 0.8191418051719666)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"messi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('atat', 0.8466954827308655),\n",
       " ('kemal', 0.8030822277069092),\n",
       " ('istanbul', 0.7852783203125),\n",
       " ('turkishminutetm', 0.7809292674064636),\n",
       " ('hurriyet', 0.7457811832427979),\n",
       " ('turkish', 0.7173997759819031),\n",
       " ('erdo', 0.7094042301177979),\n",
       " ('yeni', 0.7084641456604004),\n",
       " ('erdogan', 0.7062608599662781),\n",
       " ('kizilay', 0.702795147895813)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_word(\"ataturk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ankara', 0.6917442679405212),\n",
       " ('turkish', 0.6719129085540771),\n",
       " ('syria', 0.6188807487487793),\n",
       " ('syrian', 0.6121764779090881),\n",
       " ('tehran', 0.6115056276321411),\n",
       " ('turks', 0.6065649390220642),\n",
       " ('erdogan', 0.6037282943725586),\n",
       " ('damascus', 0.6023814082145691),\n",
       " ('tayyip', 0.5860105752944946),\n",
       " ('akinci', 0.585946261882782)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['paris', 'turkey'], negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('turkish', 0.6825993657112122),\n",
       " ('erdogan', 0.6465330719947815),\n",
       " ('davutoglu', 0.6372458934783936),\n",
       " ('yildirim', 0.6269304752349854),\n",
       " ('marashipov', 0.622066855430603),\n",
       " ('ankara', 0.6176018714904785),\n",
       " ('erdo', 0.6105588674545288),\n",
       " ('davuto', 0.5798016786575317),\n",
       " ('kremlin', 0.5581178665161133),\n",
       " ('reproaches', 0.5468676090240479)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['trump', 'turkey'], negative=['america'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('ronaldo', 0.8427572250366211),\n",
       " ('cristiano', 0.8336759805679321),\n",
       " ('atletico', 0.8255482316017151),\n",
       " ('neymar', 0.8068755269050598),\n",
       " ('higuain', 0.795189380645752),\n",
       " ('striker', 0.7914223074913025),\n",
       " ('zidane', 0.7898510694503784),\n",
       " ('atl', 0.7897554039955139),\n",
       " ('isco', 0.7802573442459106),\n",
       " ('benzema', 0.7757295966148376)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['messi', 'madrid'], negative=['barcelona'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('seine', 0.7563114166259766),\n",
       " ('rueger', 0.697585940361023),\n",
       " ('vermeer', 0.6901447772979736),\n",
       " ('ingres', 0.6818567514419556),\n",
       " ('frescoes', 0.6806928515434265),\n",
       " ('porto', 0.671973705291748),\n",
       " ('hatton', 0.6668001413345337),\n",
       " ('parthenon', 0.6627079248428345),\n",
       " ('milan', 0.6609148979187012),\n",
       " ('prizren', 0.6590593457221985)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['louvre', 'italy'], negative=['france'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(fname_or_handle=\"model/\" + \"model_for_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_vecs(column):\n",
    "    words = getattr(column, 'content').split()\n",
    "    vecs = []\n",
    "    for word in words:\n",
    "        vecs.append(model[word])\n",
    "    if len(vecs) == 0:\n",
    "        return np.zeros(50)\n",
    "    vec_sum = np.sum(vecs, axis = 0)\n",
    "    vec_avg = np.divide(vec_sum, len(vecs))\n",
    "    return vec_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_length_by_words(text):\n",
    "    return len(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_text_length_by_chars(text):\n",
    "    return len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_publisher_dictionary(df):\n",
    "    publication_array = df['publication'].unique()\n",
    "    publication_index = np.arange(len(publication_array))\n",
    "    return dict(zip(publication_array, publication_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_col_names = []\n",
    "for x in range(50):\n",
    "    vec_col_names.append('v' + str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_columns_for_model(df):\n",
    "    feature_columns = pd.DataFrame()\n",
    "    for row in df.itertuples():\n",
    "        if(type(getattr(row, 'content')) == str):\n",
    "            vecs = calculate_avg_vecs(row)\n",
    "            vec_df = pd.DataFrame([vecs.tolist()], columns=vec_col_names)\n",
    "            column = [getattr(row, 'content')]\n",
    "            col_df = pd.DataFrame([column], columns=['content'])\n",
    "            col_df['number_of_words'] = calculate_text_length_by_words(getattr(row, 'content'))\n",
    "            col_df['len_of_text'] = calculate_text_length_by_chars(getattr(row, 'content'))\n",
    "            col_df['publication'] = publisher_dict.get(getattr(row, 'publication'))\n",
    "            feature_df = pd.concat([col_df, vec_df], axis=1)\n",
    "            feature_columns = feature_columns.append(feature_df)\n",
    "    return feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/toprak.ucar/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "publisher_dict = generate_publisher_dictionary(df)\n",
    "feature_columns = generate_columns_for_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns[\"isBreitbart\"] = np.where(feature_columns['publication'] == 1, \"1\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = feature_columns.groupby('isBreitbart').apply(lambda x : x.sample(frac = 0.7))\n",
    "test = pd.concat([training,feature_columns]).drop_duplicates(subset='content',keep=False)       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "h2o.init(max_mem_size=\"10G\")\n",
    "h2o.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frame = h2o.H2OFrame(training)\n",
    "test_frame =  h2o.H2OFrame(test)\n",
    "hf = h2o.H2OFrame(test)\n",
    "test_hf, validation_hf = hf.split_frame(ratios=[0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_frame['isBreitbart'] = training_frame['isBreitbart'].asfactor()\n",
    "test_frame['isBreitbart'] = test_frame['isBreitbart'].asfactor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_col_names.extend(['number_of_words', 'len_of_text'])\n",
    "predictors = vec_col_names\n",
    "response = 'isBreitbart'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = H2ORandomForestEstimator(model_id=\"rf_model\",\n",
    "                                            ntrees=20, \n",
    "                                            max_depth=10, \n",
    "                                            nfolds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.train(x=predictors, \n",
    "                       y=response, \n",
    "                       training_frame=training_frame, \n",
    "                       validation_frame=validation_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.save_model(model=rf_model,\n",
    "                      path='model/model_name',\n",
    "                      force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = rf_model.model_performance(test_data=test_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns.to_csv(\"feature_columns.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_estimator = H2OGradientBoostingEstimator(\n",
    "    stopping_metric = 'AUC',\n",
    "    stopping_tolerance = 0.001,\n",
    "    stopping_rounds = 5,\n",
    "    score_tree_interval = 10,\n",
    "    model_id=\"id\",\n",
    "    seed=2000000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_estimator.train(predictors, response, training_frame=training_frame, validation_frame=validation_hf)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_estimator.confusion_matrix(valid=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = gradient_boosting_estimator.model_performance(test_data=test_hf)\n",
    "print(performance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
